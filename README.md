# âš™ï¸ `dataops_pipeline` â€” Airflow Setup on GCP VM

This repository runs the production orchestration for fraud detection using Apache Airflow.
The setup is designed to run manually on a clean **Debian-based VM on Google Cloud Platform**.

---

## âœ… 1. System setup (one-time, on a fresh VM)

Run the following commands after VM creation:

```bash
sudo apt-get update -y
sudo apt-get install -y git curl python3-pip
```

---

## ğŸ“¦ 2. Clone the project

```bash
git clone https://gitlab.com/automatic_fraud_detection_b3/dataops_pipeline.git
cd dataops_pipeline
```

---

## ğŸš€ 3. Run the Airflow environment setup

This script installs:

* `uv` (dependency manager)
* Apache Airflow `2.8.4`
* Google provider `10.1.1`
* and sets up the local environment

```bash
chmod +x setup_vm_airflow.sh
./setup_vm_airflow.sh
```

---

## ğŸŒ€ 4. Launch Airflow

```bash
souce .env
source .env.airflow
source .venv/bin/activate

# Start the webserver and scheduler
airflow webserver --port 8080 &
airflow scheduler &
```

Airflow UI: `http://<YOUR_VM_PUBLIC_IP>:8080`

> To kill process and restart Airflow : `pkill airflow & pkill guvicorn`, then, relaunch.
---

## ğŸŒ 5. Get your IP & open port 8080 in GCP

* From the VM:

  ```bash
  curl ifconfig.me
  ```

* In the GCP Console:
  Go to **VPC > Firewall rules**, and create a rule:

| Field            | Value                                 |
| ---------------- | ------------------------------------- |
| Name             | `allow-airflow-8080`                  |
| Targets          | All instances in the network          |
| Protocols/Ports  | TCP: `8080`                           |
| Source IP Ranges | `0.0.0.0/0` *(or restrict as needed)* |

---

## ğŸ“‚ 6. DAGs to enable in the UI

Once Airflow is running, enable the following DAGs:

* `fetch_api_data` â€” fetches real-time payments every minute
* `predict_payments` â€” runs fraud prediction batch jobs
* `monitor_fraud` â€” monitors predictions and triggers alerts

---

## âš ï¸ Requirements

Make sure the following components are configured:

* âœ… GCP **storage bucket**
* âœ… GCP **BigQuery datasets** (`raw_api_data`, `predictions`)
* âœ… GCP **service account key** (`GOOGLE_APPLICATION_CREDENTIALS`)
* âœ… **Discord webhook** for alert notifications

---